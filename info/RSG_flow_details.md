

## ğŸ”„ Complete RAG Flow 

### Timeline of What Happens:

```
Step 1: User uploads PDF via Streamlit UI
   â†“
   Files are stored in state but NOT processed yet
   â†“
Step 2: User sends first query (e.g., "What is Amazon's revenue?")
   â†“
   Graph execution starts:
   â†“
Step 3: process_documents Node (FIRST TIME)
   â†“
   âœ… Check: Does vectorstore exist on disk for these files?
   â”œâ”€ YES â†’ Load from disk (skip processing) â†’ Go to Step 4
   â””â”€ NO  â†’ Process documents:
            â”œâ”€ Load PDF â†’ Extract text
            â”œâ”€ Split into chunks
            â”œâ”€ Generate embeddings (OpenAI API calls)
            â”œâ”€ Create FAISS vectorstore
            â””â”€ ğŸ’¾ SAVE TO DISK NOW (vectorstore_db/vectorstore_{hash}/)
            â†“
            Go to Step 4
   â†“
Step 4: retrieve_context Node
   â†“
   âœ… Vectorstore is already in memory (from Step 3)
   â”œâ”€ Convert query to embedding (OpenAI API)
   â”œâ”€ Search vectorstore for similar chunks
   â””â”€ Return top K relevant chunks
   â†“
Step 5: generate_response Node
   â†“
   â”œâ”€ Combine query + retrieved context
   â”œâ”€ Generate final answer (Groq LLM)
   â””â”€ Return response to user
```

---

## ğŸ’¡ Key Points for Interview

### 1. **File Upload vs Vectorstore Creation**

- **File Upload**: Files are stored in UI session state only - no processing yet
- **First Query**: Triggers processing - vectorstore is created and saved to disk in `process_documents` node, **before** search happens

### 2. **Vectorstore is Saved Immediately After Creation**

Looking at the code flow:

```python
# In process_documents node (Line 137-143):
print("ğŸ”§ Creating and saving vector store to disk...")
self.rag_module.find_or_create_vectorstore(file_names=file_names, chunks=chunks)
# This internally calls:
#   1. create_vectorstore() - Creates embeddings and FAISS index
#   2. save_local() - Saves to disk IMMEDIATELY (Line 244)
print("âœ… Vector store created and saved to disk successfully")
```

**Vectorstore saving happens in Step 3 (`process_documents`), NOT during search (`retrieve_context`).**

### 3. **Subsequent Queries Reuse Saved Vectorstore**

```
First Query:
  File Upload â†’ process_documents (creates + saves) â†’ retrieve_context â†’ generate_response

Second Query (same files):
  No file upload â†’ process_documents (loads from disk) â†’ retrieve_context â†’ generate_response
```

### 4. **Smart Caching**

The code checks for existing vectorstore before processing:

```python
# Line 87-98: Check if vectorstore exists on disk
existing_vectorstore = self.rag_module.load_vectorstore(file_names=file_names)

if existing_vectorstore:
    print("âœ… Found existing vectorstore - using it!")
    # Skip processing - reuse saved vectorstore
    return state
```

---

## ğŸ¤ Simple Interview Explanation

**"Here's how the RAG system works:**

1. **User uploads PDF**: Files are in the UI, no processing yet.
2. **User asks first question**: This triggers document processing:
   - Extract text from PDF
   - Split into chunks
   - Generate embeddings via OpenAI API (one call per chunk)
   - Create FAISS vectorstore
   - **Save to disk immediately** at `vectorstore_db/vectorstore_{file_hash}/`
3. **Search happens**: Query is embedded and searched in the vectorstore to find relevant chunks.
4. **Answer generation**: LLM uses retrieved context to answer.

**For subsequent queries on the same files:**
- Skip processing
- Load the saved vectorstore from disk
- Perform search directly

**The vectorstore is saved during the first query processing step, not during upload or search."**

---

## ğŸ“Š Visual Flow for Interview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER UPLOADS PDF                                         â”‚
â”‚   â†’ Files stored in Streamlit session                   â”‚
â”‚   â†’ NO vectorstore created yet                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER SENDS FIRST QUERY                                  â”‚
â”‚   "What is Amazon's revenue?"                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: process_documents                             â”‚
â”‚                                                          â”‚
â”‚  â“ Check: vectorstore exists for these files?         â”‚
â”‚     â”œâ”€ NO â†’ Process:                                     â”‚
â”‚     â”‚   â”œâ”€ Load PDF text                                â”‚
â”‚     â”‚   â”œâ”€ Split into chunks                           â”‚
â”‚     â”‚   â”œâ”€ Generate embeddings (OpenAI)                â”‚
â”‚     â”‚   â”œâ”€ Create FAISS vectorstore                    â”‚
â”‚     â”‚   â””â”€ ğŸ’¾ SAVE TO DISK NOW                          â”‚
â”‚     â”‚                                                    â”‚
â”‚     â””â”€ YES â†’ Load from disk (skip processing)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: retrieve_context                               â”‚
â”‚                                                          â”‚
â”‚  âœ… Vectorstore already in memory                        â”‚
â”‚  â”œâ”€ Embed query (OpenAI)                                â”‚
â”‚  â”œâ”€ Search vectorstore                                   â”‚
â”‚  â””â”€ Return top K chunks                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: generate_response                             â”‚
â”‚                                                          â”‚
â”‚  â”œâ”€ Combine query + context                             â”‚
â”‚  â”œâ”€ Generate answer (Groq LLM)                          â”‚
â”‚  â””â”€ Return final response                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Detailed Code Flow

### When Vectorstore is Created and Saved:

**Location**: `src/langgraphagenticai/nodes/rag_node.py` â†’ `process_documents()`

```python
def process_documents(self, state: dict) -> dict:
    # ... code ...
    
    # Line 137-143: Create and save vectorstore
    print("ğŸ”§ Creating and saving vector store to disk...")
    print(f"   Creating embeddings for {len(chunks)} chunks...")
    print(f"   This will be saved to disk for future use")
    
    # Create vector store and save to disk
    self.rag_module.find_or_create_vectorstore(file_names=file_names, chunks=chunks)
    # â†‘ This creates AND saves to disk immediately
    
    self.vectorstore_created = True
    print("âœ… Vector store created and saved to disk successfully")
```

**Location**: `src/langgraphagenticai/RAG/rag_module.py` â†’ `create_vectorstore()`

```python
def create_vectorstore(self, chunks: List, file_names: Optional[List[str]] = None, save_to_disk: bool = True):
    # ... code ...
    
    # Line 226: Create FAISS vectorstore
    self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
    
    # Line 240-258: Save to disk IMMEDIATELY
    if save_to_disk and file_names:
        vectorstore_path = self.get_vectorstore_path(file_names)
        print(f"ğŸ’¾ Saving vectorstore to disk at: {vectorstore_path}")
        try:
            self.vectorstore.save_local(vectorstore_path)  # â† SAVES HERE
            print(f"âœ… Vectorstore saved successfully!")
            
            # Save metadata about files
            metadata_file = os.path.join(vectorstore_path, "metadata.json")
            metadata = {
                "file_names": file_names,
                "num_chunks": len(chunks),
                "created_at": str(time.time())
            }
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
```

### When Vectorstore is Used for Search:

**Location**: `src/langgraphagenticai/nodes/rag_node.py` â†’ `retrieve_context()`

```python
def retrieve_context(self, state: dict) -> dict:
    # Vectorstore is already in memory from process_documents step
    # No need to load or create - just use it
    
    user_query = state['messages'][0].content
    
    # Line 250: Retrieve documents using existing vectorstore
    retrieved_docs = self.rag_module.retrieve_documents(user_query, k=5)
    # â†‘ Vectorstore is already in memory, search happens here
```

---

## ğŸ” Key Takeaways

1. **Vector database is stored**: During the first query, in the `process_documents` node, immediately after creation.
2. **Not stored**: During file upload, during search, or later.
3. **Why this design**: Creates vectorstore on demand, saves it for reuse, and avoids reprocessing same files.

This approach:
- âœ… Saves API costs (embeddings generated once)
- âœ… Faster subsequent queries (reuses saved vectorstore)
- âœ… Persistent storage (survives app restarts)

---

## ğŸ¯ Some common key points

###  When is the vector database created?
**A**: The vector database is created when the user sends their first query, in the `process_documents` node. It's not created during file upload.

###  When is the vector database saved to disk?
**A**: The vector database is saved to disk immediately after creation, in the `create_vectorstore()` method, which is called during the `process_documents` step (before search).

###  Does the system reprocess documents every time?
**A**: No. The system first checks if a vectorstore exists on disk for the uploaded files. If found, it loads the existing vectorstore instead of reprocessing.

### What happens if I upload the same file twice?
**A**: The system generates a hash based on file names. If you upload the same files again, it will find the existing vectorstore and reuse it, avoiding reprocessing.

###   Where is the vector database stored?
**A**: It's stored in `./vectorstore_db/vectorstore_{file_hash}/` directory, containing:
- `index.faiss` - FAISS index with embeddings
- `index.pkl` - Document metadata and mappings
- `metadata.json` - File information and chunk count


## ğŸ¯ When is Vector Database Stored?

**A**: The vector database is stored **when the user sends their first query** (not during file upload, not during search). It happens **before search**, in the `process_documents` step.

---

## ğŸ“š Related Files

- **Node Implementation**: `src/langgraphagenticai/nodes/rag_node.py`
- **RAG Module**: `src/langgraphagenticai/RAG/rag_module.py`
- **Graph Builder**: `src/langgraphagenticai/graph/graph_builder.py`
- **State Management**: `src/langgraphagenticai/state/state.py`

---

**End of RAG Flow Explanation**

