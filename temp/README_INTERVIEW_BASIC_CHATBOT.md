# ğŸ“‹ Interview Guide: Basic Chatbot (LangGraph)

> **Comprehensive interview questions and answers about the Basic Chatbot implementation using LangGraph**

---

## ğŸ“‘ Table of Contents

1. [Architecture Overview](#-architecture-overview)
2. [Conceptual Questions](#-conceptual-questions)
3. [Implementation Questions](#-implementation-questions)
4. [Code Walkthrough](#-code-walkthrough)
5. [State Management Questions](#-state-management-questions)
6. [Advanced Questions](#-advanced-questions)
7. [Troubleshooting Questions](#-troubleshooting-questions)
8. [Best Practices](#-best-practices)

---

## ğŸ—ï¸ Architecture Overview

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Streamlit UI Layer                       â”‚
â”‚  â€¢ User Input Widget                                          â”‚
â”‚  â€¢ Message Display                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Application Layer                         â”‚
â”‚  â€¢ LoadStreamlitUI.load_streamlit_ui()                       â”‚
â”‚  â€¢ Gets user message                                          â”‚
â”‚  â€¢ Configures LLM (Groq)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Graph Builder Layer                        â”‚
â”‚  GraphBuilder.setup_graph("Basic Chatbot")                   â”‚
â”‚  â””â”€â”€> basic_chatbot_build_graph()                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LangGraph Execution                        â”‚
â”‚                                                               â”‚
â”‚   START â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º [chatbot] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º END              â”‚
â”‚                      â”‚                                        â”‚
â”‚                      â”‚ State['messages']                      â”‚
â”‚                      â”‚                                        â”‚
â”‚                      â–¼                                        â”‚
â”‚              BasicChatbotNode.process()                       â”‚
â”‚                      â”‚                                        â”‚
â”‚                      â–¼                                        â”‚
â”‚              LLM.invoke(state['messages'])                    â”‚
â”‚                      â”‚                                        â”‚
â”‚                      â–¼                                        â”‚
â”‚              Updated State['messages']                       â”‚
â”‚                      â”‚                                        â”‚
â”‚                      â–¼                                        â”‚
â”‚                   END (Response displayed)                    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Graph Structure

```
                    Graph Structure
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Entry Point: START                              â”‚
â”‚                                                  â”‚
â”‚  Node: chatbot                                   â”‚
â”‚  â””â”€â”€ Function: BasicChatbotNode.process()      â”‚
â”‚      â”œâ”€â”€ Input: State['messages']               â”‚
â”‚      â”œâ”€â”€ Process: LLM.invoke(messages)          â”‚
â”‚      â””â”€â”€ Output: Updated State['messages']       â”‚
â”‚                                                  â”‚
â”‚  Exit Point: END                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flow: Linear (START â†’ chatbot â†’ END)
No conditional routing
No tool execution
No state branching
```

### State Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Initial State                          â”‚
â”‚  {                                                         â”‚
â”‚    "messages": [HumanMessage("Hello!")]                   â”‚
â”‚  }                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BasicChatbotNode.process()                    â”‚
â”‚                                                           â”‚
â”‚  Input:  state['messages'] = [HumanMessage("Hello!")]    â”‚
â”‚                                                           â”‚
â”‚  Process:                                                 â”‚
â”‚    1. Extract messages from state                         â”‚
â”‚    2. Call: self.llm.invoke(state['messages'])            â”‚
â”‚    3. LLM generates response                              â”‚
â”‚                                                           â”‚
â”‚  Output:                                                  â”‚
â”‚    {"messages": [AIMessage("Hi there!")]}                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Final State                              â”‚
â”‚  {                                                         â”‚
â”‚    "messages": [                                          â”‚
â”‚      HumanMessage("Hello!"),                              â”‚
â”‚      AIMessage("Hi there!")                               â”‚
â”‚    ]                                                      â”‚
â”‚  }                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ Conceptual Questions

### Q1: What is LangGraph and why did you choose it for this project?

**Answer:**
LangGraph is a framework for building **stateful, multi-agent applications** with LLMs. I chose it because:

1. **State Management**: LangGraph provides built-in state management using TypedDict, making it easy to maintain conversation history across interactions.

2. **Graph-Based Workflows**: It allows me to model complex workflows as graphs with nodes and edges, making the architecture visual and maintainable.

3. **Extensibility**: Easy to add new nodes, edges, and conditional routing for future enhancements.

4. **LangChain Integration**: Seamlessly integrates with LangChain, providing access to LLMs, prompts, and message handling.

**Key Benefits:**
- **Stateful**: Unlike stateless APIs, maintains conversation context
- **Modular**: Each node is independent and testable
- **Scalable**: Easy to add new features (tools, conditional routing, etc.)

---

### Q2: How does a Basic Chatbot differ from a traditional chatbot?

**Answer:**

**Traditional Chatbot:**
```python
# Stateless - no memory
def chatbot(message):
    return llm.generate(message)  # Each call is independent
```

**LangGraph Basic Chatbot:**
```python
# Stateful - maintains conversation history
state = {"messages": [...]}  # Preserves context
result = graph.invoke(state)  # Uses previous messages
```

**Key Differences:**

| Traditional | LangGraph Basic Chatbot |
|------------|------------------------|
| Stateless | Stateful |
| No memory | Maintains conversation history |
| Single response | Context-aware responses |
| No workflow | Graph-based workflow |
| Hard to extend | Easy to add nodes/features |

---

### Q3: Explain the concept of "nodes" and "edges" in LangGraph.

**Answer:**

**Node:**
- A **function** that processes state
- Takes state as input, returns updated state
- Represents a step in the workflow

**Example:**
```python
def chatbot_node(state: State) -> dict:
    """Process user message and generate response"""
    messages = state['messages']
    response = llm.invoke(messages)
    return {"messages": [response]}  # Updated state
```

**Edge:**
- A **connection** between nodes
- Defines the flow of execution
- Can be direct or conditional

**Example:**
```python
# Direct edge
graph.add_edge("node1", "node2")  # node1 â†’ node2

# Conditional edge
def route_decision(state):
    if condition(state):
        return "node2"
    return "end"

graph.add_conditional_edges("node1", route_decision)
```

**In Basic Chatbot:**
- **Node**: `chatbot` (BasicChatbotNode.process)
- **Edges**: 
  - `START â†’ chatbot` (direct)
  - `chatbot â†’ END` (direct)

---

### Q4: What is the State object and how is it used?

**Answer:**

**State Definition:**
```python
from typing import TypedDict, Annotated, List
from langgraph.graph.message import add_messages

class State(TypedDict, total=False):
    messages: Annotated[list, add_messages]
```

**Key Points:**
1. **TypedDict**: Type-safe dictionary structure
2. **Annotated[list, add_messages]**: Automatically merges messages
3. **total=False**: All fields are optional

**State Usage:**
```python
# Initial state
state = {
    "messages": [HumanMessage("Hello!")]
}

# Node processes state
def chatbot_node(state: State) -> dict:
    # Access messages
    messages = state['messages']
    
    # Generate response
    response = llm.invoke(messages)
    
    # Return updated state
    return {"messages": [response]}  # add_messages merges this
```

**State Flow:**
```
Initial:  [HumanMessage("Hello!")]
          â†“
After:    [HumanMessage("Hello!"), AIMessage("Hi!")]
```

---

### Q5: Why is the Basic Chatbot linear (no branching)?

**Answer:**

**Basic Chatbot is linear because:**
1. **Simple Use Case**: Only needs to generate a response - no tools, no conditional logic
2. **Single Path**: Always follows the same flow: User â†’ LLM â†’ Response
3. **No Decision Points**: Doesn't need to choose between multiple paths

**Graph Structure:**
```
START â†’ [chatbot] â†’ END
```

**When would you add branching?**
- **Tool Usage**: Add conditional edge to route to tools
- **Error Handling**: Route to error handler on failure
- **Multi-step Processing**: Route through multiple processing nodes

**Example (non-linear):**
```
START â†’ [chatbot] â†’ [conditional_edge]
                      â”œâ”€â†’ [tools] â†’ [chatbot] â†’ END
                      â””â”€â†’ END
```

---

## ğŸ”§ Implementation Questions

### Q6: Walk me through the code structure of Basic Chatbot.

**Answer:**

**1. Graph Builder (`graph/graph_builder.py`):**
```python
class GraphBuilder:
    def __init__(self, model):
        self.llm = model
        self.graph_builder = StateGraph(State)  # Create graph
    
    def basic_chatbot_build_graph(self):
        # Create node instance
        self.basic_chatbot_node = BasicChatbotNode(self.llm)
        
        # Add node to graph
        self.graph_builder.add_node(
            "chatbot", 
            self.basic_chatbot_node.process
        )
        
        # Add edges
        self.graph_builder.add_edge(START, "chatbot")
        self.graph_builder.add_edge("chatbot", END)
```

**2. Node Implementation (`nodes/basic_chatbot_node.py`):**
```python
class BasicChatbotNode:
    def __init__(self, model):
        self.llm = model  # Groq LLM instance
    
    def process(self, state: State) -> dict:
        # Process state and generate response
        return {"messages": self.llm.invoke(state['messages'])}
```

**3. Execution Flow:**
```python
# Initialize
graph_builder = GraphBuilder(model)

# Build graph
graph_builder.basic_chatbot_build_graph()

# Compile
graph = graph_builder.graph_builder.compile()

# Execute
initial_state = {"messages": [HumanMessage("Hello!")]}
result = graph.invoke(initial_state)
```

---

### Q7: How does the LLM integration work?

**Answer:**

**LLM Setup:**
```python
# In GroqLLM class
from langchain_groq import ChatGroq

class GroqLLM:
    def get_llm_model(self):
        llm = ChatGroq(
            api_key=groq_api_key,
            model="llama3-70b-8192"
        )
        return llm
```

**LLM Invocation:**
```python
# In BasicChatbotNode.process()
def process(self, state: State) -> dict:
    # state['messages'] is a list of LangChain messages
    # LLM processes messages and generates response
    response = self.llm.invoke(state['messages'])
    
    # Response is automatically merged into state['messages']
    return {"messages": [response]}
```

**Message Format:**
```python
# LangChain message format
from langchain_core.messages import HumanMessage, AIMessage

messages = [
    HumanMessage(content="Hello!"),
    AIMessage(content="Hi there!")
]
```

**Why LangChain Messages?**
- Standardized format across LLM providers
- Handles different message types (Human, AI, System, Tool)
- Automatic state merging with `add_messages`

---

### Q8: Explain the `add_messages` annotation and how it works.

**Answer:**

**Definition:**
```python
from langgraph.graph.message import add_messages

messages: Annotated[list, add_messages]
```

**Purpose:**
- Automatically merges new messages into existing message list
- Prevents message duplication
- Maintains conversation history

**How it Works:**

```python
# Initial state
state = {
    "messages": [HumanMessage("Hello!")]
}

# Node returns new message
new_state = {"messages": [AIMessage("Hi!")]}

# add_messages automatically merges
# Result:
state['messages'] = [
    HumanMessage("Hello!"),
    AIMessage("Hi!")
]
```

**Without `add_messages`:**
```python
# Would overwrite:
state['messages'] = [AIMessage("Hi!")]  # Loses "Hello!"
```

**With `add_messages`:**
```python
# Automatically appends:
state['messages'] = [
    HumanMessage("Hello!"),
    AIMessage("Hi!")
]
```

---

### Q9: How is the graph compiled and executed?

**Answer:**

**Graph Compilation:**
```python
# After adding nodes and edges
graph = self.graph_builder.compile()
```

**What happens during compilation:**
1. Validates graph structure (nodes, edges)
2. Builds execution plan
3. Creates optimized execution engine

**Graph Execution:**

**Option 1: `invoke()` (synchronous)**
```python
initial_state = {"messages": [HumanMessage("Hello!")]}
result = graph.invoke(initial_state)

# Returns final state
# {
#   "messages": [
#     HumanMessage("Hello!"),
#     AIMessage("Hi there!")
#   ]
# }
```

**Option 2: `stream()` (async, for UI updates)**
```python
for event in graph.stream(initial_state):
    # Event format: {"node_name": {"field": value}}
    print(event)
    # {"chatbot": {"messages": [AIMessage("Hi!")]}}
```

**Execution Steps:**
1. Entry: START
2. Node: Execute `chatbot` node
3. State Update: Merge response into state
4. Exit: END

---

### Q10: How does this integrate with Streamlit UI?

**Answer:**

**UI Flow:**
```python
# 1. Load UI (loadui.py)
ui = LoadStreamlitUI()
user_input = ui.load_streamlit_ui()
# Gets: API key, model selection, use case

# 2. User sends message
user_message = st.chat_input("Enter your message:")

# 3. Configure LLM (main.py)
llm_config = GroqLLM(user_input)
model = llm_config.get_llm_model()

# 4. Build graph
graph_builder = GraphBuilder(model)
if usecase == "Basic Chatbot":
    graph_builder.basic_chatbot_build_graph()

# 5. Compile graph
graph = graph_builder.graph_builder.compile()

# 6. Execute (display_result.py)
initial_state = {
    "messages": [HumanMessage(user_message)]
}
result = graph.invoke(initial_state)

# 7. Display result
st.chat_message("assistant").write(
    result['messages'][-1].content
)
```

**State Management in UI:**
```python
# Maintain conversation history
if 'messages' not in st.session_state:
    st.session_state.messages = []

# Add user message
st.session_state.messages.append(
    HumanMessage(user_message)
)

# Execute graph
result = graph.invoke({
    "messages": st.session_state.messages
})

# Add AI response
st.session_state.messages.append(
    result['messages'][-1]
)
```

---

## ğŸ’¾ State Management Questions

### Q11: How is state passed between nodes?

**Answer:**

**State Flow:**
```python
# Node 1 receives state
def node1(state: State) -> dict:
    # Process state
    return {"field1": "value1"}

# LangGraph automatically merges returned dict into state
# Updated state passed to next node
```

**Automatic Merging:**
```python
# Initial state
state = {"messages": [HumanMessage("Hello!")]}

# Node returns
return {"messages": [AIMessage("Hi!")]}

# LangGraph merges (using add_messages)
# Final state:
{
    "messages": [
        HumanMessage("Hello!"),
        AIMessage("Hi!")
    ]
}
```

**Key Points:**
- State is immutable between nodes (LangGraph creates new state)
- Returned dict is merged into state
- `add_messages` handles message list merging
- Other fields are overwritten (not merged)

---

### Q12: What happens if a node returns an empty dict?

**Answer:**

**Scenario:**
```python
def node(state: State) -> dict:
    return {}  # Empty dict
```

**Result:**
- State remains unchanged
- Next node receives same state
- No errors raised (valid behavior)

**Use Cases:**
- Conditional nodes that don't always update state
- Validation nodes that only return errors
- Skip nodes that pass through state unchanged

**Example:**
```python
def validate_node(state: State) -> dict:
    if not state.get('messages'):
        return {"error": "No messages"}
    return {}  # Valid, pass through
```

---

### Q13: How is conversation history maintained across multiple interactions?

**Answer:**

**In Streamlit:**
```python
# Session state maintains conversation
if 'messages' not in st.session_state:
    st.session_state.messages = []

# Add user message
st.session_state.messages.append(
    HumanMessage(user_input)
)

# Execute graph with full history
result = graph.invoke({
    "messages": st.session_state.messages
})

# Add AI response
st.session_state.messages.append(
    result['messages'][-1]
)
```

**In LangGraph:**
- Each execution receives full message history
- `add_messages` appends new messages
- No built-in persistence (state is ephemeral)

**For Persistence:**
- Store messages in database (MongoDB, PostgreSQL)
- Use session storage (Redis)
- Save to file (JSON)

**Example with Persistence:**
```python
# Load previous messages
previous_messages = load_from_db(session_id)

# Execute with history
result = graph.invoke({
    "messages": previous_messages + [HumanMessage(user_input)]
})

# Save updated history
save_to_db(session_id, result['messages'])
```

---

## ğŸš€ Advanced Questions

### Q14: How would you add error handling to Basic Chatbot?

**Answer:**

**Option 1: Try-except in Node**
```python
def process(self, state: State) -> dict:
    try:
        response = self.llm.invoke(state['messages'])
        return {"messages": [response]}
    except Exception as e:
        return {
            "messages": [AIMessage("Sorry, an error occurred.")],
            "error": str(e)
        }
```

**Option 2: Error Node with Conditional Routing**
```python
# Add error handling node
def error_node(state: State) -> dict:
    return {
        "messages": [AIMessage("An error occurred. Please try again.")]
    }

# Add conditional edge
def route_decision(state):
    if state.get('error'):
        return "error_handler"
    return "chatbot"

graph.add_conditional_edges("chatbot", route_decision)
graph.add_edge("error_handler", END)
```

**Option 3: Validation Node**
```python
def validate_node(state: State) -> dict:
    if not state.get('messages'):
        return {"error": "No messages to process"}
    return {}

# Graph structure
# START â†’ validate â†’ chatbot â†’ END
#                â†“ (if error)
#            error_handler â†’ END
```

---

### Q15: How would you add rate limiting or token usage tracking?

**Answer:**

**Rate Limiting:**
```python
from collections import defaultdict
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, max_requests=10, window_minutes=1):
        self.requests = defaultdict(list)
        self.max_requests = max_requests
        self.window = timedelta(minutes=window_minutes)
    
    def is_allowed(self, user_id: str) -> bool:
        now = datetime.now()
        user_requests = self.requests[user_id]
        
        # Remove old requests
        user_requests = [
            req_time for req_time in user_requests
            if now - req_time < self.window
        ]
        
        if len(user_requests) >= self.max_requests:
            return False
        
        user_requests.append(now)
        self.requests[user_id] = user_requests
        return True

# In node
rate_limiter = RateLimiter()

def process(self, state: State) -> dict:
    user_id = state.get('user_id')
    if not rate_limiter.is_allowed(user_id):
        return {
            "messages": [AIMessage("Rate limit exceeded. Please wait.")]
        }
    
    response = self.llm.invoke(state['messages'])
    return {"messages": [response]}
```

**Token Usage Tracking:**
```python
def process(self, state: State) -> dict:
    response = self.llm.invoke(state['messages'])
    
    # Get token usage from response
    token_usage = response.response_metadata.get('token_usage', {})
    
    return {
        "messages": [response],
        "token_usage": {
            "prompt_tokens": token_usage.get('prompt_tokens', 0),
            "completion_tokens": token_usage.get('completion_tokens', 0),
            "total_tokens": token_usage.get('total_tokens', 0)
        }
    }
```

---

### Q16: How would you optimize the Basic Chatbot for production?

**Answer:**

**1. Caching:**
```python
from functools import lru_cache
from hashlib import md5

@lru_cache(maxsize=100)
def cached_response(message_hash: str):
    # Cache common responses
    pass

def process(self, state: State) -> dict:
    message_hash = md5(
        str(state['messages']).encode()
    ).hexdigest()
    
    if cached_response:
        return {"messages": [cached_response(message_hash)]}
    
    response = self.llm.invoke(state['messages'])
    cache_response(message_hash, response)
    return {"messages": [response]}
```

**2. Async Execution:**
```python
import asyncio

async def process_async(self, state: State) -> dict:
    response = await self.llm.ainvoke(state['messages'])
    return {"messages": [response]}
```

**3. Connection Pooling:**
```python
# Reuse LLM instances
class LLMPool:
    def __init__(self):
        self.pool = []
    
    def get_llm(self):
        if self.pool:
            return self.pool.pop()
        return ChatGroq(...)
    
    def return_llm(self, llm):
        self.pool.append(llm)
```

**4. Logging and Monitoring:**
```python
import logging

logger = logging.getLogger(__name__)

def process(self, state: State) -> dict:
    logger.info(f"Processing {len(state['messages'])} messages")
    start_time = time.time()
    
    response = self.llm.invoke(state['messages'])
    
    duration = time.time() - start_time
    logger.info(f"Response generated in {duration:.2f}s")
    
    return {"messages": [response]}
```

---

## ğŸ› Troubleshooting Questions

### Q17: What if the LLM doesn't respond?

**Answer:**

**Possible Causes:**
1. API key invalid or expired
2. Rate limit exceeded
3. Network timeout
4. Model unavailable

**Solutions:**

**1. Validate API Key:**
```python
def process(self, state: State) -> dict:
    try:
        response = self.llm.invoke(state['messages'])
        return {"messages": [response]}
    except ValueError as e:
        if "API key" in str(e):
            return {
                "messages": [AIMessage("Invalid API key. Please check your configuration.")],
                "error": "API_KEY_ERROR"
            }
        raise
```

**2. Retry Logic:**
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))
def process(self, state: State) -> dict:
    response = self.llm.invoke(state['messages'])
    return {"messages": [response]}
```

**3. Timeout Handling:**
```python
import signal

def timeout_handler(signum, frame):
    raise TimeoutError("LLM response timeout")

def process(self, state: State) -> dict:
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(30)  # 30 second timeout
    
    try:
        response = self.llm.invoke(state['messages'])
        signal.alarm(0)  # Cancel alarm
        return {"messages": [response]}
    except TimeoutError:
        return {
            "messages": [AIMessage("Request timed out. Please try again.")]
        }
```

---

### Q18: How do you debug state issues in LangGraph?

**Answer:**

**1. Add Debug Logging:**
```python
def process(self, state: State) -> dict:
    print(f"ğŸ” State keys: {list(state.keys())}")
    print(f"ğŸ” Messages count: {len(state.get('messages', []))}")
    print(f"ğŸ” Last message: {state['messages'][-1] if state.get('messages') else 'None'}")
    
    response = self.llm.invoke(state['messages'])
    
    print(f"ğŸ” Response type: {type(response)}")
    print(f"ğŸ” Response content: {response.content[:100]}")
    
    return {"messages": [response]}
```

**2. State Inspection:**
```python
def process(self, state: State) -> dict:
    # Validate state structure
    assert 'messages' in state, "State missing 'messages' field"
    assert isinstance(state['messages'], list), "Messages must be a list"
    assert len(state['messages']) > 0, "No messages in state"
    
    response = self.llm.invoke(state['messages'])
    return {"messages": [response]}
```

**3. Use LangGraph Debugging:**
```python
# Enable debug mode
graph = graph_builder.graph_builder.compile(debug=True)

# Or use checkpointing
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
graph = graph_builder.graph_builder.compile(checkpointer=checkpointer)
```

**4. State Visualization:**
```python
import json

def process(self, state: State) -> dict:
    # Log state as JSON
    print(json.dumps(state, indent=2, default=str))
    
    response = self.llm.invoke(state['messages'])
    return {"messages": [response]}
```

---

## âœ… Best Practices

### Q19: What are the best practices for Basic Chatbot implementation?

**Answer:**

**1. Error Handling:**
```python
def process(self, state: State) -> dict:
    try:
        response = self.llm.invoke(state['messages'])
        return {"messages": [response]}
    except Exception as e:
        logger.error(f"Error in chatbot: {e}", exc_info=True)
        return {
            "messages": [AIMessage("I'm sorry, I encountered an error. Please try again.")],
            "error": str(e)
        }
```

**2. Input Validation:**
```python
def process(self, state: State) -> dict:
    if not state.get('messages'):
        return {"error": "No messages provided"}
    
    if len(state['messages']) == 0:
        return {"error": "Empty messages list"}
    
    response = self.llm.invoke(state['messages'])
    return {"messages": [response]}
```

**3. Logging:**
```python
import logging

logger = logging.getLogger(__name__)

def process(self, state: State) -> dict:
    logger.info("Processing chatbot request")
    logger.debug(f"State: {state}")
    
    response = self.llm.invoke(state['messages'])
    
    logger.info("Response generated successfully")
    return {"messages": [response]}
```

**4. Type Hints:**
```python
from typing import Dict, Any

def process(self, state: State) -> Dict[str, Any]:
    # Type hints improve code clarity and IDE support
    response = self.llm.invoke(state['messages'])
    return {"messages": [response]}
```

**5. Configuration:**
```python
class BasicChatbotNode:
    def __init__(self, model, max_tokens: int = 1000, temperature: float = 0.7):
        self.llm = model
        self.max_tokens = max_tokens
        self.temperature = temperature
    
    def process(self, state: State) -> dict:
        # Use configuration
        response = self.llm.invoke(
            state['messages'],
            max_tokens=self.max_tokens,
            temperature=self.temperature
        )
        return {"messages": [response]}
```

---

## ğŸ“ Summary

### Key Takeaways

1. **LangGraph Basics**: Graph-based stateful workflows with nodes and edges
2. **State Management**: TypedDict with `add_messages` for automatic message merging
3. **Linear Flow**: Simple chatbot uses linear graph (START â†’ chatbot â†’ END)
4. **Extensibility**: Easy to add features like error handling, validation, logging
5. **Integration**: Seamlessly integrates with Streamlit for UI

### Code Structure
```
GraphBuilder
  â””â”€â”€ basic_chatbot_build_graph()
      â”œâ”€â”€ Create BasicChatbotNode
      â”œâ”€â”€ Add "chatbot" node
      â”œâ”€â”€ Add edges (START â†’ chatbot â†’ END)
      â””â”€â”€ Compile graph

BasicChatbotNode
  â””â”€â”€ process(state)
      â”œâ”€â”€ Extract messages from state
      â”œâ”€â”€ Call LLM.invoke(messages)
      â””â”€â”€ Return updated state
```

### Interview Tips

- **Understand State Flow**: Explain how state is passed and merged
- **Graph Structure**: Draw the graph structure (START â†’ node â†’ END)
- **LLM Integration**: Explain how LangChain messages work
- **Extensibility**: Show how to add features (error handling, validation)
- **Production Ready**: Discuss optimizations (caching, logging, error handling)

---

**End of Interview Guide: Basic Chatbot**

For more interview questions, see:
- `README_INTERVIEW_CHATBOT_WITH_TOOLS.md`
- `README_INTERVIEW_RAG_CHATBOT.md`

